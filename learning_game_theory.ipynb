{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Learning Game Theory: Multi-Agent Reinforcement Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook demonstrates how **Multi-Agent Reinforcement Learning (MARL)** can discover game-theoretic equilibria through trial and error. We'll show how learning agents can discover Nash equilibria in Cournot and Bertrand competition models without knowing the theoretical solutions.\n",
        "\n",
        "## Key Questions We'll Explore\n",
        "\n",
        "1. **Can learning agents discover Nash equilibria?**\n",
        "2. **How fast do they converge to optimal strategies?**\n",
        "3. **How do learned strategies compare to theoretical predictions?**\n",
        "4. **What happens when agents use different learning algorithms?**\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you'll understand:\n",
        "- How Q-learning works in multi-agent settings\n",
        "- The relationship between MARL and game theory\n",
        "- Convergence properties of learning algorithms\n",
        "- Performance comparison between different learning approaches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# Set up plotting style with Rose Pine Dawn theme (light background)\n",
        "plt.style.use('rose-pine-dawn')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready to explore learning in game theory!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Q-Learning Agent Implementation\n",
        "\n",
        "Let's start by implementing a Q-learning agent that can learn optimal strategies in multi-agent environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "    \"\"\"\n",
        "    A Q-learning agent that learns optimal strategies in multi-agent environments.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, agent_id, learning_rate=0.1, discount_factor=0.9, \n",
        "                 epsilon=0.1, epsilon_decay=0.995, epsilon_min=0.01):\n",
        "        self.agent_id = agent_id\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        \n",
        "        # Q-table: state -> action -> value\n",
        "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
        "        \n",
        "        # Learning history\n",
        "        self.episode_rewards = []\n",
        "        self.strategy_history = []\n",
        "        \n",
        "    def get_state(self, game_state):\n",
        "        \"\"\"Convert game state to a hashable state representation.\"\"\"\n",
        "        # For now, use opponent's last action as state\n",
        "        return tuple(game_state)\n",
        "    \n",
        "    def choose_action(self, state, available_actions):\n",
        "        \"\"\"Choose action using epsilon-greedy policy.\"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            # Explore: random action\n",
        "            action = random.choice(available_actions)\n",
        "        else:\n",
        "            # Exploit: best known action\n",
        "            q_values = [self.q_table[state][action] for action in available_actions]\n",
        "            best_value = max(q_values)\n",
        "            best_actions = [action for action, value in zip(available_actions, q_values) \n",
        "                          if value == best_value]\n",
        "            action = random.choice(best_actions)\n",
        "        \n",
        "        return action\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, available_actions):\n",
        "        \"\"\"Update Q-values using Q-learning update rule.\"\"\"\n",
        "        # Current Q-value\n",
        "        current_q = self.q_table[state][action]\n",
        "        \n",
        "        # Maximum Q-value for next state\n",
        "        if available_actions:\n",
        "            max_next_q = max([self.q_table[next_state][next_action] \n",
        "                            for next_action in available_actions])\n",
        "        else:\n",
        "            max_next_q = 0\n",
        "        \n",
        "        # Q-learning update\n",
        "        new_q = current_q + self.learning_rate * (\n",
        "            reward + self.discount_factor * max_next_q - current_q\n",
        "        )\n",
        "        \n",
        "        self.q_table[state][action] = new_q\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decay exploration rate over time.\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "    \n",
        "    def get_strategy(self, state, available_actions):\n",
        "        \"\"\"Get current strategy (action probabilities) for a state.\"\"\"\n",
        "        if not available_actions:\n",
        "            return {}\n",
        "        \n",
        "        q_values = [self.q_table[state][action] for action in available_actions]\n",
        "        max_q = max(q_values)\n",
        "        \n",
        "        # Softmax strategy based on Q-values\n",
        "        exp_values = np.exp(np.array(q_values) - max_q)\n",
        "        probabilities = exp_values / np.sum(exp_values)\n",
        "        \n",
        "        return dict(zip(available_actions, probabilities))\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset agent for new learning session.\"\"\"\n",
        "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
        "        self.episode_rewards = []\n",
        "        self.strategy_history = []\n",
        "        self.epsilon = 0.1  # Reset epsilon\n",
        "\n",
        "print(\"QLearningAgent class implemented successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cournot Competition Learning\n",
        "\n",
        "Now let's implement a Cournot game environment where Q-learning agents can discover the Nash equilibrium quantities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CournotGame:\n",
        "    \"\"\"\n",
        "    Cournot competition game environment for learning agents.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, a=100, b=1, c=10, max_quantity=50):\n",
        "        \"\"\"\n",
        "        Initialize Cournot game.\n",
        "        \n",
        "        Args:\n",
        "            a: Demand intercept\n",
        "            b: Demand slope  \n",
        "            c: Marginal cost\n",
        "            max_quantity: Maximum quantity each firm can produce\n",
        "        \"\"\"\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.c = c\n",
        "        self.max_quantity = max_quantity\n",
        "        self.available_actions = list(range(0, max_quantity + 1))\n",
        "        \n",
        "    def calculate_payoff(self, quantities):\n",
        "        \"\"\"Calculate payoff for each firm given quantities.\"\"\"\n",
        "        total_quantity = sum(quantities)\n",
        "        price = max(0, self.a - self.b * total_quantity)  # Market price\n",
        "        \n",
        "        payoffs = []\n",
        "        for q in quantities:\n",
        "            if total_quantity > 0:  # Avoid division by zero\n",
        "                profit = (price - self.c) * q\n",
        "            else:\n",
        "                profit = 0\n",
        "            payoffs.append(profit)\n",
        "        \n",
        "        return payoffs\n",
        "    \n",
        "    def get_nash_equilibrium(self, n_firms=2):\n",
        "        \"\"\"Calculate theoretical Nash equilibrium.\"\"\"\n",
        "        q_star = (self.a - self.c) / (self.b * (n_firms + 1))\n",
        "        return [q_star] * n_firms\n",
        "    \n",
        "    def get_monopoly_quantity(self):\n",
        "        \"\"\"Calculate monopoly quantity.\"\"\"\n",
        "        return (self.a - self.c) / (2 * self.b)\n",
        "    \n",
        "    def get_competitive_quantity(self, n_firms=2):\n",
        "        \"\"\"Calculate competitive quantity per firm.\"\"\"\n",
        "        return (self.a - self.c) / (self.b * n_firms)\n",
        "\n",
        "def simulate_cournot_learning(episodes=1000, n_firms=2):\n",
        "    \"\"\"\n",
        "    Simulate Q-learning agents learning to play Cournot competition.\n",
        "    \"\"\"\n",
        "    # Initialize game and agents\n",
        "    game = CournotGame()\n",
        "    agents = [QLearningAgent(i, learning_rate=0.1, epsilon=0.2) \n",
        "              for i in range(n_firms)]\n",
        "    \n",
        "    # Learning history\n",
        "    episode_rewards = [[] for _ in range(n_firms)]\n",
        "    quantity_history = [[] for _ in range(n_firms)]\n",
        "    price_history = []\n",
        "    \n",
        "    # Theoretical benchmarks\n",
        "    nash_quantities = game.get_nash_equilibrium(n_firms)\n",
        "    nash_price = game.a - game.b * sum(nash_quantities)\n",
        "    \n",
        "    print(f\"Starting Cournot learning simulation...\")\n",
        "    print(f\"Theoretical Nash equilibrium: {[f'{q:.1f}' for q in nash_quantities]}\")\n",
        "    print(f\"Theoretical Nash price: {nash_price:.1f}\")\n",
        "    \n",
        "    for episode in range(episodes):\n",
        "        # Initialize state (opponents' previous actions)\n",
        "        state = tuple([0] * n_firms)  # Start with zero quantities\n",
        "        \n",
        "        # Agents choose quantities\n",
        "        actions = []\n",
        "        for i, agent in enumerate(agents):\n",
        "            action = agent.choose_action(state, game.available_actions)\n",
        "            actions.append(action)\n",
        "        \n",
        "        # Calculate payoffs\n",
        "        payoffs = game.calculate_payoff(actions)\n",
        "        total_quantity = sum(actions)\n",
        "        price = max(0, game.a - game.b * total_quantity)\n",
        "        \n",
        "        # Update agents\n",
        "        for i, agent in enumerate(agents):\n",
        "            # Create next state (other agents' current actions)\n",
        "            next_state = tuple(actions[:i] + actions[i+1:])\n",
        "            \n",
        "            # Update Q-values\n",
        "            agent.update(state, actions[i], payoffs[i], next_state, \n",
        "                        game.available_actions)\n",
        "            \n",
        "            # Record history\n",
        "            episode_rewards[i].append(payoffs[i])\n",
        "            quantity_history[i].append(actions[i])\n",
        "        \n",
        "        price_history.append(price)\n",
        "        \n",
        "        # Decay exploration\n",
        "        if episode % 100 == 0:\n",
        "            for agent in agents:\n",
        "                agent.decay_epsilon()\n",
        "        \n",
        "        # Print progress\n",
        "        if episode % 200 == 0:\n",
        "            avg_quantities = [np.mean(quantity_history[i][-100:]) \n",
        "                            for i in range(n_firms)]\n",
        "            avg_price = np.mean(price_history[-100:])\n",
        "            avg_epsilon = np.mean([agent.epsilon for agent in agents])\n",
        "            \n",
        "            print(f\"Episode {episode}: Avg quantities = {[f'{q:.1f}' for q in avg_quantities]}, \"\n",
        "                  f\"Avg price = {avg_price:.1f}, Epsilon = {avg_epsilon:.3f}\")\n",
        "    \n",
        "    return {\n",
        "        'agents': agents,\n",
        "        'episode_rewards': episode_rewards,\n",
        "        'quantity_history': quantity_history,\n",
        "        'price_history': price_history,\n",
        "        'nash_quantities': nash_quantities,\n",
        "        'nash_price': nash_price,\n",
        "        'game': game\n",
        "    }\n",
        "\n",
        "print(\"CournotGame class and simulation function implemented!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Cournot learning simulation\n",
        "results = simulate_cournot_learning(episodes=1000, n_firms=2)\n",
        "\n",
        "print(\"\\n=== LEARNING RESULTS ===\")\n",
        "print(f\"Final learned quantities: {[np.mean(results['quantity_history'][i][-100:]) for i in range(2)]}\")\n",
        "print(f\"Final average price: {np.mean(results['price_history'][-100:]):.1f}\")\n",
        "print(f\"Theoretical Nash quantities: {[f'{q:.1f}' for q in results['nash_quantities']]}\")\n",
        "print(f\"Theoretical Nash price: {results['nash_price']:.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Bertrand Competition Learning\n",
        "\n",
        "Now let's implement Bertrand competition where agents learn to set prices instead of quantities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BertrandGame:\n",
        "    \"\"\"\n",
        "    Bertrand competition game environment for learning agents.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, a=100, b=1, c=10, max_price=100):\n",
        "        \"\"\"\n",
        "        Initialize Bertrand game.\n",
        "        \n",
        "        Args:\n",
        "            a: Demand intercept\n",
        "            b: Demand slope  \n",
        "            c: Marginal cost\n",
        "            max_price: Maximum price each firm can set\n",
        "        \"\"\"\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.c = c\n",
        "        self.max_price = max_price\n",
        "        self.available_actions = list(range(c, max_price + 1))  # Prices >= marginal cost\n",
        "        \n",
        "    def calculate_payoff(self, prices):\n",
        "        \"\"\"Calculate payoff for each firm given prices.\"\"\"\n",
        "        payoffs = []\n",
        "        \n",
        "        for i, price_i in enumerate(prices):\n",
        "            # Check if this firm has the lowest price\n",
        "            if price_i == min(prices):\n",
        "                # If tied for lowest price, split the market\n",
        "                lowest_price_count = prices.count(price_i)\n",
        "                if lowest_price_count > 1:\n",
        "                    # Split market equally among tied firms\n",
        "                    quantity = (self.a - self.b * price_i) / lowest_price_count\n",
        "                else:\n",
        "                    # Firm gets entire market\n",
        "                    quantity = self.a - self.b * price_i\n",
        "            else:\n",
        "                # Firm gets no demand\n",
        "                quantity = 0\n",
        "            \n",
        "            profit = (price_i - self.c) * quantity\n",
        "            payoffs.append(max(0, profit))  # Ensure non-negative profits\n",
        "        \n",
        "        return payoffs\n",
        "    \n",
        "    def get_nash_equilibrium(self, n_firms=2):\n",
        "        \"\"\"Calculate theoretical Nash equilibrium (Bertrand paradox).\"\"\"\n",
        "        return [self.c] * n_firms  # All firms set price = marginal cost\n",
        "    \n",
        "    def get_monopoly_price(self):\n",
        "        \"\"\"Calculate monopoly price.\"\"\"\n",
        "        return (self.a + self.c) / (2 * self.b)\n",
        "\n",
        "def simulate_bertrand_learning(episodes=1000, n_firms=2):\n",
        "    \"\"\"\n",
        "    Simulate Q-learning agents learning to play Bertrand competition.\n",
        "    \"\"\"\n",
        "    # Initialize game and agents\n",
        "    game = BertrandGame()\n",
        "    agents = [QLearningAgent(i, learning_rate=0.1, epsilon=0.2) \n",
        "              for i in range(n_firms)]\n",
        "    \n",
        "    # Learning history\n",
        "    episode_rewards = [[] for _ in range(n_firms)]\n",
        "    price_history = [[] for _ in range(n_firms)]\n",
        "    quantity_history = []\n",
        "    \n",
        "    # Theoretical benchmarks\n",
        "    nash_prices = game.get_nash_equilibrium(n_firms)\n",
        "    monopoly_price = game.get_monopoly_price()\n",
        "    \n",
        "    print(f\"Starting Bertrand learning simulation...\")\n",
        "    print(f\"Theoretical Nash equilibrium: {nash_prices} (Bertrand Paradox)\")\n",
        "    print(f\"Theoretical monopoly price: {monopoly_price:.1f}\")\n",
        "    \n",
        "    for episode in range(episodes):\n",
        "        # Initialize state (opponents' previous actions)\n",
        "        state = tuple([game.c] * n_firms)  # Start with marginal cost\n",
        "        \n",
        "        # Agents choose prices\n",
        "        actions = []\n",
        "        for i, agent in enumerate(agents):\n",
        "            action = agent.choose_action(state, game.available_actions)\n",
        "            actions.append(action)\n",
        "        \n",
        "        # Calculate payoffs\n",
        "        payoffs = game.calculate_payoff(actions)\n",
        "        \n",
        "        # Calculate market outcomes\n",
        "        total_quantity = sum([(game.a - game.b * min(actions)) if min(actions) == p else 0 \n",
        "                             for p in actions])\n",
        "        market_price = min(actions)\n",
        "        \n",
        "        # Update agents\n",
        "        for i, agent in enumerate(agents):\n",
        "            # Create next state (other agents' current actions)\n",
        "            next_state = tuple(actions[:i] + actions[i+1:])\n",
        "            \n",
        "            # Update Q-values\n",
        "            agent.update(state, actions[i], payoffs[i], next_state, \n",
        "                        game.available_actions)\n",
        "            \n",
        "            # Record history\n",
        "            episode_rewards[i].append(payoffs[i])\n",
        "            price_history[i].append(actions[i])\n",
        "        \n",
        "        quantity_history.append(total_quantity)\n",
        "        \n",
        "        # Decay exploration\n",
        "        if episode % 100 == 0:\n",
        "            for agent in agents:\n",
        "                agent.decay_epsilon()\n",
        "        \n",
        "        # Print progress\n",
        "        if episode % 200 == 0:\n",
        "            avg_prices = [np.mean(price_history[i][-100:]) \n",
        "                         for i in range(n_firms)]\n",
        "            avg_quantity = np.mean(quantity_history[-100:])\n",
        "            avg_epsilon = np.mean([agent.epsilon for agent in agents])\n",
        "            \n",
        "            print(f\"Episode {episode}: Avg prices = {[f'{p:.1f}' for p in avg_prices]}, \"\n",
        "                  f\"Avg quantity = {avg_quantity:.1f}, Epsilon = {avg_epsilon:.3f}\")\n",
        "    \n",
        "    return {\n",
        "        'agents': agents,\n",
        "        'episode_rewards': episode_rewards,\n",
        "        'price_history': price_history,\n",
        "        'quantity_history': quantity_history,\n",
        "        'nash_prices': nash_prices,\n",
        "        'monopoly_price': monopoly_price,\n",
        "        'game': game\n",
        "    }\n",
        "\n",
        "print(\"BertrandGame class and simulation function implemented!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Convergence Analysis and Visualizations\n",
        "\n",
        "Let's create comprehensive visualizations to analyze how well the learning agents converge to theoretical equilibria.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_learning_convergence(cournot_results, bertrand_results):\n",
        "    \"\"\"\n",
        "    Create comprehensive visualizations of learning convergence.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    \n",
        "    # Cournot Learning Plots\n",
        "    # Plot 1: Quantity convergence over time\n",
        "    episodes = range(len(cournot_results['quantity_history'][0]))\n",
        "    for i in range(2):\n",
        "        # Moving average for smoother visualization\n",
        "        window = 50\n",
        "        moving_avg = pd.Series(cournot_results['quantity_history'][i]).rolling(window).mean()\n",
        "        axes[0, 0].plot(episodes, moving_avg, label=f'Firm {i+1}', alpha=0.8)\n",
        "    \n",
        "    # Add theoretical Nash equilibrium\n",
        "    axes[0, 0].axhline(y=cournot_results['nash_quantities'][0], \n",
        "                      color='red', linestyle='--', linewidth=2, \n",
        "                      label='Nash Equilibrium')\n",
        "    axes[0, 0].set_xlabel('Episode')\n",
        "    axes[0, 0].set_ylabel('Quantity')\n",
        "    axes[0, 0].set_title('Cournot: Quantity Convergence')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Price convergence over time\n",
        "    window = 50\n",
        "    moving_avg_price = pd.Series(cournot_results['price_history']).rolling(window).mean()\n",
        "    axes[0, 1].plot(episodes, moving_avg_price, color='green', alpha=0.8, label='Market Price')\n",
        "    axes[0, 1].axhline(y=cournot_results['nash_price'], \n",
        "                      color='red', linestyle='--', linewidth=2, \n",
        "                      label='Nash Price')\n",
        "    axes[0, 1].set_xlabel('Episode')\n",
        "    axes[0, 1].set_ylabel('Price')\n",
        "    axes[0, 1].set_title('Cournot: Price Convergence')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Reward convergence\n",
        "    for i in range(2):\n",
        "        window = 50\n",
        "        moving_avg_reward = pd.Series(cournot_results['episode_rewards'][i]).rolling(window).mean()\n",
        "        axes[0, 2].plot(episodes, moving_avg_reward, label=f'Firm {i+1}', alpha=0.8)\n",
        "    \n",
        "    axes[0, 2].set_xlabel('Episode')\n",
        "    axes[0, 2].set_ylabel('Profit')\n",
        "    axes[0, 2].set_title('Cournot: Profit Convergence')\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Bertrand Learning Plots\n",
        "    # Plot 4: Price convergence over time\n",
        "    episodes_bertrand = range(len(bertrand_results['price_history'][0]))\n",
        "    for i in range(2):\n",
        "        window = 50\n",
        "        moving_avg = pd.Series(bertrand_results['price_history'][i]).rolling(window).mean()\n",
        "        axes[1, 0].plot(episodes_bertrand, moving_avg, label=f'Firm {i+1}', alpha=0.8)\n",
        "    \n",
        "    # Add theoretical Nash equilibrium (marginal cost)\n",
        "    axes[1, 0].axhline(y=bertrand_results['nash_prices'][0], \n",
        "                      color='red', linestyle='--', linewidth=2, \n",
        "                      label='Nash Equilibrium (MC)')\n",
        "    axes[1, 0].set_xlabel('Episode')\n",
        "    axes[1, 0].set_ylabel('Price')\n",
        "    axes[1, 0].set_title('Bertrand: Price Convergence')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 5: Quantity convergence over time\n",
        "    window = 50\n",
        "    moving_avg_quantity = pd.Series(bertrand_results['quantity_history']).rolling(window).mean()\n",
        "    axes[1, 1].plot(episodes_bertrand, moving_avg_quantity, color='green', alpha=0.8, label='Market Quantity')\n",
        "    \n",
        "    # Calculate theoretical competitive quantity\n",
        "    competitive_quantity = bertrand_results['game'].a - bertrand_results['game'].b * bertrand_results['nash_prices'][0]\n",
        "    axes[1, 1].axhline(y=competitive_quantity, \n",
        "                      color='red', linestyle='--', linewidth=2, \n",
        "                      label='Competitive Quantity')\n",
        "    axes[1, 1].set_xlabel('Episode')\n",
        "    axes[1, 1].set_ylabel('Quantity')\n",
        "    axes[1, 1].set_title('Bertrand: Quantity Convergence')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 6: Reward convergence\n",
        "    for i in range(2):\n",
        "        window = 50\n",
        "        moving_avg_reward = pd.Series(bertrand_results['episode_rewards'][i]).rolling(window).mean()\n",
        "        axes[1, 2].plot(episodes_bertrand, moving_avg_reward, label=f'Firm {i+1}', alpha=0.8)\n",
        "    \n",
        "    # Bertrand equilibrium has zero profits\n",
        "    axes[1, 2].axhline(y=0, color='red', linestyle='--', linewidth=2, label='Nash Profits (0)')\n",
        "    axes[1, 2].set_xlabel('Episode')\n",
        "    axes[1, 2].set_ylabel('Profit')\n",
        "    axes[1, 2].set_title('Bertrand: Profit Convergence')\n",
        "    axes[1, 2].legend()\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_convergence_performance(cournot_results, bertrand_results):\n",
        "    \"\"\"\n",
        "    Analyze and compare convergence performance.\n",
        "    \"\"\"\n",
        "    print(\"=== CONVERGENCE ANALYSIS ===\")\n",
        "    \n",
        "    # Cournot Analysis\n",
        "    print(\"\\n📊 COURNOT COMPETITION:\")\n",
        "    final_quantities = [np.mean(cournot_results['quantity_history'][i][-100:]) \n",
        "                       for i in range(2)]\n",
        "    final_price = np.mean(cournot_results['price_history'][-100:])\n",
        "    \n",
        "    nash_quantities = cournot_results['nash_quantities']\n",
        "    nash_price = cournot_results['nash_price']\n",
        "    \n",
        "    quantity_errors = [abs(final_quantities[i] - nash_quantities[i]) \n",
        "                      for i in range(2)]\n",
        "    price_error = abs(final_price - nash_price)\n",
        "    \n",
        "    print(f\"  Learned quantities: {[f'{q:.1f}' for q in final_quantities]}\")\n",
        "    print(f\"  Nash quantities:    {[f'{q:.1f}' for q in nash_quantities]}\")\n",
        "    print(f\"  Quantity errors:    {[f'{e:.1f}' for e in quantity_errors]}\")\n",
        "    print(f\"  Learned price:      {final_price:.1f}\")\n",
        "    print(f\"  Nash price:         {nash_price:.1f}\")\n",
        "    print(f\"  Price error:        {price_error:.1f}\")\n",
        "    \n",
        "    # Bertrand Analysis\n",
        "    print(\"\\n💰 BERTRAND COMPETITION:\")\n",
        "    final_prices = [np.mean(bertrand_results['price_history'][i][-100:]) \n",
        "                   for i in range(2)]\n",
        "    final_quantity = np.mean(bertrand_results['quantity_history'][-100:])\n",
        "    \n",
        "    nash_prices = bertrand_results['nash_prices']\n",
        "    competitive_quantity = bertrand_results['game'].a - bertrand_results['game'].b * nash_prices[0]\n",
        "    \n",
        "    price_errors = [abs(final_prices[i] - nash_prices[i]) for i in range(2)]\n",
        "    quantity_error = abs(final_quantity - competitive_quantity)\n",
        "    \n",
        "    print(f\"  Learned prices:     {[f'{p:.1f}' for p in final_prices]}\")\n",
        "    print(f\"  Nash prices:        {nash_prices}\")\n",
        "    print(f\"  Price errors:       {[f'{e:.1f}' for e in price_errors]}\")\n",
        "    print(f\"  Learned quantity:   {final_quantity:.1f}\")\n",
        "    print(f\"  Competitive qty:    {competitive_quantity:.1f}\")\n",
        "    print(f\"  Quantity error:     {quantity_error:.1f}\")\n",
        "    \n",
        "    # Overall Assessment\n",
        "    print(\"\\n🎯 LEARNING ASSESSMENT:\")\n",
        "    avg_cournot_error = np.mean(quantity_errors + [price_error])\n",
        "    avg_bertrand_error = np.mean(price_errors + [quantity_error])\n",
        "    \n",
        "    print(f\"  Average Cournot error: {avg_cournot_error:.2f}\")\n",
        "    print(f\"  Average Bertrand error: {avg_bertrand_error:.2f}\")\n",
        "    \n",
        "    if avg_cournot_error < 5 and avg_bertrand_error < 5:\n",
        "        print(\"  ✅ Excellent convergence to theoretical equilibria!\")\n",
        "    elif avg_cournot_error < 10 and avg_bertrand_error < 10:\n",
        "        print(\"  ✅ Good convergence to theoretical equilibria!\")\n",
        "    else:\n",
        "        print(\"  ⚠️  Learning could be improved with more episodes or different parameters.\")\n",
        "\n",
        "print(\"Convergence analysis functions implemented!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run both learning simulations\n",
        "print(\"🚀 Starting Learning Game Theory Analysis...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Run Cournot learning\n",
        "cournot_results = simulate_cournot_learning(episodes=1000, n_firms=2)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Run Bertrand learning  \n",
        "bertrand_results = simulate_bertrand_learning(episodes=1000, n_firms=2)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Create convergence visualizations\n",
        "plot_learning_convergence(cournot_results, bertrand_results)\n",
        "\n",
        "# Analyze convergence performance\n",
        "analyze_convergence_performance(cournot_results, bertrand_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
